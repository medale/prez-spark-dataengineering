{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark Data Engineering 1\n",
    "\n",
    "## Data source - https://www.gharchive.org/\n",
    "\n",
    "* Hourly download since 1/1/2015 using GitHub Event API - all events. About 40MB/hour * 37900 hours ~ 1.5TB.\n",
    "\n",
    "We'll be working with the following subset:\n",
    "```bash\n",
    "wget http://data.gharchive.org/2019-04-28-0.json.gz\n",
    "wget http://data.gharchive.org/2019-04-28-1.json.gz\n",
    "wget http://data.gharchive.org/2019-04-28-13.json.gz\n",
    "```\n",
    "\n",
    "## Mission: Allow data scientists to explore pull requests for anomalies\n",
    "* [GitHub Developer PullRequestEvent](https://developer.github.com/v3/activity/events/types/#pullrequestevent)\n",
    "* Out of all events, store just pull requests using efficient storage format\n",
    "* Allow for fast time-based access when specifying yyyy, mm, dd, hh (or prefix combination)\n",
    "\n",
    "## Read and explore\n",
    "\n",
    "* We use local file system that is mounted on the driver and executor at `/datasets/github/data`.\n",
    "     * For production - distributed file systems: \n",
    "     * In cloud: AWS Simple Storage Service (S3)\n",
    "     * On prem: Hadoop Distributed File System (HDFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "records = [actor: struct<avatar_url: string, display_login: string ... 4 more fields>, created_at: string ... 6 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[actor: struct<avatar_url: string, display_login: string ... 4 more fields>, created_at: string ... 6 more fields]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val records = spark.read.json(\"/datasets/github/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147374"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records.cache\n",
    "records.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records.unpersist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
